{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')  # create blank Language class\n",
    "print(\"Created blank 'en' model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['REFNO', 'PNR', 'TTIME', 'SRC', 'DES', 'AGENT', 'PASSENGER', 'FLTNO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the parser to the pipeline if it doesn't exist\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "\n",
    "if 'parser' not in nlp.pipe_names:\n",
    "    parser = nlp.create_pipe('parser')\n",
    "    nlp.add_pipe(parser, first=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    parser = nlp.get_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    ner.add_label(label)\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n"
     ]
    }
   ],
   "source": [
    "if model is None:\n",
    "    optimizer = nlp.begin_training()\n",
    "else:\n",
    "    # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "    # existing entity types.\n",
    "    optimizer = nlp.entity.create_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    ## Booking ID Samples\n",
    "    \n",
    "    (\"Booking Id: NF22696100810981\", {\n",
    "        'entities': [(12, 28, 'REFNO')]\n",
    "    }),\n",
    "\n",
    "    (\"Booking Id: NF22696100810981\", {\n",
    "        'entities': [(12, 28, 'REFNO')]\n",
    "    }),\n",
    "\n",
    "    (\"Booking Id: NF22695116846185\", {\n",
    "        'entities': [(12, 28, 'REFNO')]\n",
    "    }),\n",
    "\n",
    "    (\"Booking Id: NF22995132466090\", {\n",
    "        'entities': [(12, 28, 'REFNO')]\n",
    "    }),\n",
    "\n",
    "    (\"Booking Id: NF22996135146142\", {\n",
    "        'entities': [(12, 28, 'REFNO')]\n",
    "    }),\n",
    "    \n",
    "    ## Time and Date Samples\n",
    "\n",
    "    (\"17:00 hrs, 18 Mar\", {\n",
    "        'entities': [(0, 17, 'TTIME')]\n",
    "    }),\n",
    "\n",
    "    (\"19:45 hrs, 18 Mar\", {\n",
    "        'entities': [(0, 17, 'TTIME')]\n",
    "    }),\n",
    "\n",
    "    (\"07:15 hrs, 03 Feb\", {\n",
    "        'entities': [(0, 17, 'TTIME')]\n",
    "    }),\n",
    "\n",
    "    (\"10:05 hrs, 03 Feb\", {\n",
    "        'entities': [(0, 17, 'TTIME')]\n",
    "    }),\n",
    "\n",
    "    (\"08:35 hrs, 18 Aug\", {\n",
    "        'entities': [(0, 17, 'TTIME')]\n",
    "    }),\n",
    "\n",
    "    (\"10:55 hrs, 18 Aug\", {\n",
    "        'entities': [(0, 17, 'TTIME')]\n",
    "    }),\n",
    "    \n",
    "    ## Flight Number Samples\n",
    "\n",
    "    (\"G8-118\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"G8-557\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"G8-2847\", {\n",
    "        'entities': [(0, 7, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"G8-284/ 4427\", {\n",
    "        'entities': [(0, 12, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"6E-3794/ 154\", {\n",
    "        'entities': [(0, 12, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "\n",
    "    (\"6E-606\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "\n",
    "    (\"G8-557\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "\n",
    "    (\"SG-185\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"6E-528\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"I5-247\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"I5-722\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"G6-142\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"G6-411\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"UK-817\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "    (\"AI-503\", {\n",
    "        'entities': [(0, 6, 'FLTNO')]\n",
    "    }),\n",
    "\n",
    "\n",
    "    (\"1. Shahrukh Khan, Adult\", {\n",
    "        'entities': [(3, 16, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"2. Ramesh Natarajan, Adult\", {\n",
    "        'entities': [(3, 19, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"1. Ranbir Kapoor, Adult\", {\n",
    "        'entities': [(3, 16, 'PASSENGER')]\n",
    "    }),\n",
    "    (\"2. Vivek Kumar, Child\", {\n",
    "        'entities': [(3, 14, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"2. Manohar P, Adult\", {\n",
    "        'entities': [(3, 12, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"1. Vinay I, Child\", {\n",
    "        'entities': [(3, 10, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"3. Kavita Venkat, Adult\", {\n",
    "        'entities': [(3, 16, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"3. Nitin Datta, Adult\", {\n",
    "        'entities': [(3, 14, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"1. Aditya Khar, Child\", {\n",
    "        'entities': [(3, 14, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"2. Abhishek Khare, Adult\", {\n",
    "        'entities': [(3, 17, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"3. Manish Sharma, Adult\", {\n",
    "        'entities': [(3, 16, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"4. Virat Kohli, Child\", {\n",
    "        'entities': [(3, 14, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"2. Vivek Kumar, Child\", {\n",
    "        'entities': [(3, 14, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    (\"2. Mr. MS Dhoni, Adult\", {\n",
    "        'entities': [(3, 15, 'PASSENGER')]\n",
    "    }),\n",
    "\n",
    "    ## Source and Destination\n",
    "    \n",
    "    (\"DELHI TO PUNE\", {\n",
    "        'entities': [(0, 5, 'SRC'), (9, 13, 'DES')]\n",
    "    }),\n",
    "\n",
    "    (\"PUNE TO DELHI\", {\n",
    "        'entities': [(0, 4, 'SRC'), (8, 13, 'DES')]\n",
    "    }),\n",
    "\n",
    "    (\"BANGALORE TO CHENNAI\", {\n",
    "        'entities': [(0, 9, 'SRC'), (13, 20, 'DES')]\n",
    "    }),\n",
    "\n",
    "    (\"DELHI TO CHENNAI \", {\n",
    "        'entities': [(0, 5, 'SRC'), (9, 16, 'DES')]\n",
    "    }),\n",
    "\n",
    "    (\"CHENNAI TO DELHI\", {\n",
    "        'entities': [(0, 7, 'SRC'), (11, 16, 'DES')]\n",
    "    }),\n",
    "    \n",
    "    (\"EDYTGT\", {\n",
    "        'entities': [(0, 6, 'PNR')]\n",
    "    }),\n",
    "\n",
    "    (\"AGARTALA TO JAIPUR\", {\n",
    "        'entities': [(0, 8, 'SRC'), (12, 18, 'DES')]\n",
    "    }),\n",
    "    \n",
    "    ## PNR no. samples\n",
    "\n",
    "    (\"ZH17NW\", {\n",
    "        'entities': [(0, 6, 'PNR')]\n",
    "    }),\n",
    "\n",
    "    (\"E758YP\", {\n",
    "        'entities': [(0, 6, 'PNR')]\n",
    "    }),\n",
    "\n",
    "    (\"MI7WUQ\", {\n",
    "        'entities': [(0, 6, 'PNR')]\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.4865384628035122}\n",
      "Losses {'ner': 0.25775649883419605}\n",
      "Losses {'ner': 0.0646258525680429}\n",
      "Losses {'ner': 2.8932435333819317e-05}\n",
      "Losses {'ner': 0.07439277803996651}\n",
      "Losses {'ner': 0.0002331116880010124}\n",
      "Losses {'ner': 0.11948947655922801}\n",
      "Losses {'ner': 2.0293938753382582e-05}\n",
      "Losses {'ner': 0.011089605017763017}\n",
      "Losses {'ner': 3.1763118646344514e-08}\n",
      "Losses {'ner': 2.1275904832153723e-11}\n",
      "Losses {'ner': 0.001316615893689609}\n",
      "Losses {'ner': 0.09674176801769263}\n",
      "Losses {'ner': 0.0027293836074012484}\n",
      "Losses {'ner': 0.12499685592427243}\n",
      "Losses {'ner': 0.007722765623325879}\n",
      "Losses {'ner': 6.514743453469244e-13}\n",
      "Losses {'ner': 0.009697374375991606}\n",
      "Losses {'ner': 0.10343323699448624}\n",
      "Losses {'ner': 6.410270944430178e-09}\n"
     ]
    }
   ],
   "source": [
    "n_iter = 20\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35,\n",
    "                       losses=losses)\n",
    "        print('Losses', losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLTNO AI-48\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"AI-48\")\n",
    "for ent in doc1.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLTNO SG-185\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"SG-185\")\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLTNO 6E-1854\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"6E-1854\")\n",
    "for ent in doc3.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC PUNE\n",
      "DES CHENNAI\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(\"PUNE TO CHENNAI\")\n",
    "for ent in doc4.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC CHANDIGARH\n",
      "DES HYDERABAD\n"
     ]
    }
   ],
   "source": [
    "doc5 = nlp(\"CHANDIGARH TO HYDERABAD\")\n",
    "for ent in doc5.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC DELHI\n",
      "DES BANGALORE\n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(\"DELHI TO BANGALORE\")\n",
    "for ent in doc6.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTIME Brian\n"
     ]
    }
   ],
   "source": [
    "doc7 = nlp(\"2. Brian Lara, Adult\")\n",
    "for ent in doc7.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNR Ricky\n"
     ]
    }
   ],
   "source": [
    "doc8 = nlp(\"1. Ricky Ponting, Adult\")\n",
    "for ent in doc8.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNR Sachin\n"
     ]
    }
   ],
   "source": [
    "doc9 = nlp(\"3. Sachin Tendulkar, Adult\")\n",
    "for ent in doc9.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNR KB6RAE\n"
     ]
    }
   ],
   "source": [
    "doc10 = nlp(\"KB6RAE\")\n",
    "for ent in doc10.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNR PWRHSO\n"
     ]
    }
   ],
   "source": [
    "doc11 = nlp(\"PWRHSO\")\n",
    "for ent in doc11.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNR KB6RAE\n"
     ]
    }
   ],
   "source": [
    "doc12 = nlp(\"KB6RAE\")\n",
    "for ent in doc12.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REFNO NF22696100810981\n"
     ]
    }
   ],
   "source": [
    "doc13 = nlp(\"Booking Id: NF22696100810981\")\n",
    "for ent in doc13.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REFNO NF2204240022047\n"
     ]
    }
   ],
   "source": [
    "doc14 = nlp(\"Booking Id: NF2204240022047\")\n",
    "for ent in doc14.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REFNO NF2202139868170\n"
     ]
    }
   ],
   "source": [
    "doc15 = nlp(\"Booking Id: NF2202139868170\")\n",
    "for ent in doc15.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
